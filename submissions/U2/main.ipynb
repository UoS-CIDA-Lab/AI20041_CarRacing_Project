{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(img):\n",
    "  img = cv2.resize(img, dsize=(84, 84))\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnvironment(gym.Wrapper):\n",
    "  def __init__(self, env, skip_frames=2, stack_frames=4, no_operation=5, **kwargs):\n",
    "    super().__init__(env, **kwargs)\n",
    "    self._no_operation = no_operation\n",
    "    self._skip_frames = skip_frames\n",
    "    self._stack_frames = stack_frames\n",
    "\n",
    "  def reset(self):\n",
    "    observation, info = self.env.reset()\n",
    "\n",
    "    for i in range(self._no_operation):\n",
    "      observation, reward, terminated, truncated, info = self.env.step(0)\n",
    "\n",
    "    observation = image_preprocessing(observation)\n",
    "    self.stack_state = np.tile(observation, (self._stack_frames, 1, 1))\n",
    "    return self.stack_state, info\n",
    "\n",
    "\n",
    "  def step(self, action):\n",
    "    total_reward = 0\n",
    "    for i in range(self._skip_frames):\n",
    "      observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "      total_reward += reward\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "    observation = image_preprocessing(observation)\n",
    "    self.stack_state = np.concatenate((self.stack_state[1:], observation[np.newaxis]), axis=0)\n",
    "    return self.stack_state, total_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self._n_features = 32 * 9 * 9\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(self._n_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, out_channels),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv(x)\n",
    "    x = x.view((-1, self._n_features))\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "  def __init__(self, action_space, batch_size=256, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, lr=0.001):\n",
    "    self._n_observation = 4\n",
    "    self._n_actions = 5\n",
    "    self._action_space = action_space\n",
    "    self._batch_size = batch_size\n",
    "    self._gamma = gamma\n",
    "    self._eps_start = eps_start\n",
    "    self._eps_end = eps_end\n",
    "    self._eps_decay = eps_decay\n",
    "    self._lr = lr\n",
    "    self._total_steps = 0\n",
    "    self._evaluate_loss = []\n",
    "    self.network = CNN(self._n_observation, self._n_actions).to(device)\n",
    "    self.target_network = CNN(self._n_observation, self._n_actions).to(device)\n",
    "    self.target_network.load_state_dict(self.network.state_dict())\n",
    "    self.optimizer = optim.AdamW(self.network.parameters(), lr=self._lr, amsgrad=True)\n",
    "    self._memory = ReplayMemory(10000)\n",
    "\n",
    "  \"\"\"\n",
    "  This function is called during training & evaluation phase when the agent\n",
    "  interact with the environment and needs to select an action.\n",
    "\n",
    "  (1) Exploitation: This function feeds the neural network a state\n",
    "  and then it selects the action with the highest Q-value.\n",
    "  (2) Evaluation mode: This function feeds the neural network a state\n",
    "  and then it selects the action with the highest Q'-value.\n",
    "  (3) Exploration mode: It randomly selects an action through sampling\n",
    "\n",
    "  Q -> network (policy)\n",
    "  Q'-> target network (best policy)\n",
    "  \"\"\"\n",
    "  def select_action(self, state, evaluation_phase=False):\n",
    "\n",
    "    # Generating a random number for eploration vs exploitation\n",
    "    sample = random.random()\n",
    "\n",
    "    # Calculating the threshold - the more steps the less exploration we do\n",
    "    eps_threshold = self._eps_end + (self._eps_start - self._eps_end) * math.exp(-1. * self._total_steps / self._eps_decay)\n",
    "    self._total_steps += 1\n",
    "\n",
    "    if evaluation_phase:\n",
    "      with torch.no_grad():\n",
    "        return self.target_network(state).max(1).indices.view(1, 1)\n",
    "    elif sample > eps_threshold:\n",
    "      with torch.no_grad():\n",
    "        return self.network(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "      return torch.tensor([[self._action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "  def train(self):\n",
    "\n",
    "    if len(self._memory) < self._batch_size:\n",
    "        return\n",
    "\n",
    "    # Initializing our memory\n",
    "    transitions = self._memory.sample(self._batch_size)\n",
    "\n",
    "    # Initializing our batch\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Saving in a new tensor all the indices of the states that are non terminal\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "\n",
    "    # Saving in a new tensor all the non final next states\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Feeding our Q network the batch with states and then we gather the Q values of the selected actions\n",
    "    state_action_values = self.network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # We then, for every state in the batch that is NOT final, we pass it in the target network to get the Q'-values and choose the max one\n",
    "    next_state_values = torch.zeros(self._batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
    "\n",
    "    # Computing the expecting values with: reward + gamma * max(Q')\n",
    "    expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
    "\n",
    "    # Defining our loss criterion\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Updating with back propagation\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
    "    self.optimizer.step()\n",
    "\n",
    "    self._evaluate_loss.append(loss.item())\n",
    "\n",
    "    return\n",
    "\n",
    "  def copy_weights(self):\n",
    "    self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "  def get_loss(self):\n",
    "    return self._evaluate_loss\n",
    "\n",
    "  def save_model(self, i):\n",
    "    torch.save(self.target_network.state_dict(), f'model_weights_{i}.pth')\n",
    "\n",
    "  def load_model(self, i):\n",
    "    self.target_network.load_state_dict(torch.load(f'model_weights_{i}.pth', remap_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    def __init__(self, action_space, batch_size=1024, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=1000, lr=0.002, model_learning_steps=5):\n",
    "        self._n_observation = 4\n",
    "        self._n_actions = 5\n",
    "        self._action_space = action_space\n",
    "        self._batch_size = batch_size\n",
    "        self._gamma = gamma\n",
    "        self._eps_start = eps_start\n",
    "        self._eps_end = eps_end\n",
    "        self._eps_decay = eps_decay\n",
    "        self._lr = lr\n",
    "        self._total_steps = 0\n",
    "        self._evaluate_loss = []  # Stores per-iteration losses\n",
    "        self.network = CNN(self._n_observation, self._n_actions).to(device)\n",
    "        self.target_network = CNN(self._n_observation, self._n_actions).to(device)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.optimizer = optim.AdamW(self.network.parameters(), lr=self._lr, amsgrad=True)\n",
    "        self._memory = ReplayMemory(60000)\n",
    "        self._model_learning_steps = model_learning_steps  # Number of simulated experiences for Dyna-Q\n",
    "        self.model = {}  # Stores the learned environment model\n",
    "\n",
    "    def select_action(self, state, evaluation_phase=False):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self._eps_end + (self._eps_start - self._eps_end) * math.exp(-1. * self._total_steps / self._eps_decay)\n",
    "        self._total_steps += 1\n",
    "\n",
    "        if evaluation_phase:\n",
    "            with torch.no_grad():\n",
    "                return self.target_network(state).max(1).indices.view(1, 1)\n",
    "        elif sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.network(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self._action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "    def train(self):\n",
    "        if len(self._memory) < self._batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self._memory.sample(self._batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        state_action_values = self.network(state_batch).gather(1, action_batch)\n",
    "        next_state_values = torch.zeros(self._batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1).values\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self._gamma) + reward_batch\n",
    "\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self._evaluate_loss.append(loss.item())\n",
    "\n",
    "        # Perform Dyna-Q model learning and simulation\n",
    "        self.simulate_experiences()\n",
    "\n",
    "    def simulate_experiences(self):\n",
    "        for _ in range(self._model_learning_steps):\n",
    "            if len(self.model) == 0:\n",
    "                break\n",
    "\n",
    "            state = random.choice(list(self.model.keys()))\n",
    "            action = random.choice(list(self.model[state].keys()))\n",
    "            next_state, reward = self.model[state][action]\n",
    "\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0) if next_state is not None else None\n",
    "            reward_tensor = torch.tensor([reward], device=device)\n",
    "\n",
    "            self._memory.push(state_tensor, torch.tensor([[action]], device=device), next_state_tensor, reward_tensor)\n",
    "\n",
    "    def update_model(self, state, action, next_state, reward):\n",
    "        state_tuple = tuple(state.cpu().numpy().flatten())\n",
    "        next_state_tuple = tuple(next_state.cpu().numpy().flatten()) if next_state is not None else None\n",
    "\n",
    "        if state_tuple not in self.model:\n",
    "            self.model[state_tuple] = {}\n",
    "        self.model[state_tuple][action.item()] = (next_state_tuple, reward.item())\n",
    "\n",
    "    def copy_weights(self):\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "    def get_loss(self):\n",
    "        # Return losses if available, otherwise return a default value\n",
    "        return self._evaluate_loss if self._evaluate_loss else [0.0]\n",
    "\n",
    "    def save_model(self, i):\n",
    "        torch.save(self.target_network.state_dict(), f'model_weights_{i}.pth')\n",
    "\n",
    "    def load_model(self, i):\n",
    "        self.target_network.load_state_dict(torch.load(f'model_weights_{i}.pth', map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/3000 [01:38<9:42:30, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/3000 [03:37<9:47:54, 11.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 29/3000 [05:39<10:01:49, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 39/3000 [07:42<10:03:52, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 49/3000 [09:46<10:08:35, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 59/3000 [11:50<10:05:03, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 67/3000 [13:26<9:24:54, 11.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 69/3000 [13:51<9:50:32, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 73/3000 [14:38<9:27:13, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 76/3000 [15:14<9:33:28, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 79/3000 [15:46<8:26:32, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n",
      "80 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 80/3000 [15:52<7:20:36,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 81/3000 [16:02<7:41:53,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 84/3000 [16:38<8:53:16, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 87/3000 [17:14<9:12:04, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 88/3000 [17:20<7:58:08,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 89/3000 [17:30<8:00:42,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n",
      "90 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 93/3000 [18:19<9:12:23, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 96/3000 [18:52<8:35:36, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 99/3000 [19:27<8:56:49, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n",
      "100 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/3000 [19:35<8:12:11, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 104/3000 [20:19<8:00:50,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 105/3000 [20:26<7:13:38,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 108/3000 [21:03<8:48:10, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 109/3000 [21:12<8:24:26, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n",
      "110 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 115/3000 [22:25<8:55:01, 11.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 116/3000 [22:34<8:22:49, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 117/3000 [22:40<7:19:26,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 118/3000 [22:48<6:56:17,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 119/3000 [23:01<8:01:38, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 120/3000 [23:06<6:53:06,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 121/3000 [23:13<6:17:51,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 122/3000 [23:20<6:04:34,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 125/3000 [23:53<7:21:37,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 129/3000 [24:48<10:05:52, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 episodes done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 130/3000 [24:55<8:52:43, 11.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished the lap successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 134/3000 [25:50<10:25:09, 13.09s/it]"
     ]
    }
   ],
   "source": [
    "rewards_per_episode = []\n",
    "episode_duration = []\n",
    "average_episode_loss = []\n",
    "\n",
    "episodes = 3000\n",
    "C = 5\n",
    "\n",
    "env = gym.make('CarRacing-v2', lap_complete_percent=0.95, continuous=False)\n",
    "n_actions = env.action_space\n",
    "agent = DynaQ(n_actions)\n",
    "\n",
    "for episode in tqdm(range(1, episodes + 1)):\n",
    "\n",
    "  if episode % 10 == 0:\n",
    "    print(f\"{episode} episodes done\")\n",
    "\n",
    "  env = gym.make('CarRacing-v2', continuous=False)\n",
    "  env = CarEnvironment(env)\n",
    "\n",
    "  state, info = env.reset()\n",
    "\n",
    "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "  episode_total_reward = 0\n",
    "\n",
    "  for t in count():\n",
    "    action = agent.select_action(state)\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    episode_total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "      next_state = None\n",
    "      print(\"Finished the lap successfully!\")\n",
    "    else:\n",
    "      next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    agent._memory.push(state, action, next_state, reward)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    agent.train()\n",
    "\n",
    "    if done:\n",
    "      if agent._memory.__len__() >= 128:\n",
    "        episode_duration.append(t + 1)\n",
    "        rewards_per_episode.append(episode_total_reward)\n",
    "        ll = agent.get_loss()\n",
    "        #print(f\"ll: {ll}\")\n",
    "        average_episode_loss.append(sum(ll) / len(ll))\n",
    "      break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "      agent.save_model(episode)\n",
    "      with open('statistics.pkl', 'wb') as f:\n",
    "        pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)\n",
    "\n",
    "\n",
    "  if episode % C == 0:\n",
    "    agent.copy_weights()\n",
    "\n",
    "agent.save_model(episodes)\n",
    "with open('statistics.pkl', 'wb') as f:\n",
    "  pickle.dump((episode_duration, rewards_per_episode, average_episode_loss), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistics(x, y, title, x_axis, y_axis):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel(y_axis)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{title.replace(\" \", \"_\")}.png')  # 공백 대신 밑줄 사용\n",
    "    plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = CarEnvironment(eval_env)\n",
    "n_actions = eval_env.action_space\n",
    "agent = DynaQ(n_actions)\n",
    "agent.load_model(3000)\n",
    "\n",
    "frames = []\n",
    "scores = 0\n",
    "s, _ = eval_env.reset()\n",
    "\n",
    "eval_env.np_random = np.random.default_rng(42)\n",
    "\n",
    "done, ret = False, 0\n",
    "\n",
    "from PIL import Image as PILImage\n",
    "def render2img(_img): return PILImage.fromarray(_img, \"RGB\")\n",
    "handle = display(None, display_id=True)\n",
    "while not done:\n",
    "    _render = eval_env.render()\n",
    "    handle.update(render2img(_render))\n",
    "    frames.append(_render)\n",
    "    s = torch.tensor(s, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    a = agent.select_action(s, evaluation_phase=True)\n",
    "    discrete_action = a.item() % 5\n",
    "    s_prime, r, terminated, truncated, info = eval_env.step(discrete_action)\n",
    "    s = s_prime\n",
    "    ret += r\n",
    "    done = terminated or truncated\n",
    "    if terminated:\n",
    "      print(terminated)\n",
    "      \n",
    "scores += ret\n",
    "\n",
    "print(scores)\n",
    "def animate(imgs, video_name, _return=True):\n",
    "    import cv2\n",
    "    import os\n",
    "    import string\n",
    "    import random\n",
    "\n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "\n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate(frames, None)\n",
    "\n",
    "with open('statistics.pkl', 'rb') as f:\n",
    "    data_tuple = pickle.load(f)\n",
    "\n",
    "episode_duration, rewards_per_episode, average_episode_loss = data_tuple\n",
    "\n",
    "x = [k for k in range(299)]\n",
    "\n",
    "rewards_per_episode = [tensor.cpu() if tensor.is_cuda else tensor for tensor in rewards_per_episode]\n",
    "\n",
    "plot_statistics(x, rewards_per_episode, \"Rewards for every episode\", \"Episode\", \"Reward\")\n",
    "plot_statistics(x, average_episode_loss, \"Average loss for every episode\", \"Episode\", \"Average Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
